{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f21c85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fincantatem extension is not loaded.\n"
     ]
    }
   ],
   "source": [
    "%unload_ext fincantatem\n",
    "%load_ext fincantatem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65664a24",
   "metadata": {},
   "source": [
    "## Example: Shape Mismatch in Neural Network Forward Pass\n",
    "\n",
    "This example demonstrates how the JAX integration helps debug shape mismatches.\n",
    "The error message alone says \"incompatible shapes\" - but **which** arrays? **What** shapes?\n",
    "\n",
    "With the JAX context, you immediately see all array shapes in each frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387b58ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dot_general requires contracting dimensions to have the same shape, got (784,) and (128,).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m params = (w1, b1, w2, b2)\n\u001b[32m     24\u001b[39m batch = jnp.ones((\u001b[32m32\u001b[39m, \u001b[32m784\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mmlp_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mmlp_forward\u001b[39m\u001b[34m(params, x)\u001b[39m\n\u001b[32m      7\u001b[39m w1, b1, w2, b2 = params\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Hidden layer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m h = \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw1\u001b[49m\u001b[43m)\u001b[49m + b1\n\u001b[32m     11\u001b[39m h = jax.nn.relu(h)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Output layer\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 14 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/fincantatem/.venv/lib/python3.12/site-packages/jax/_src/numpy/tensor_contractions.py:122\u001b[39m, in \u001b[36mdot\u001b[39m\u001b[34m(a, b, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m    120\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    121\u001b[39m     contract_dims = ((a_ndim - \u001b[32m1\u001b[39m,), (b_ndim - \u001b[32m2\u001b[39m,))\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m result = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontract_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dims\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreferred_element_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_sharding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m lax._convert_element_type(result, preferred_element_type,\n\u001b[32m    127\u001b[39m                                  output_weak_type)\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/fincantatem/.venv/lib/python3.12/site-packages/jax/_src/lax/lax.py:5339\u001b[39m, in \u001b[36m_dot_general_shape_rule\u001b[39m\u001b[34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[39m\n\u001b[32m   5336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core.definitely_equal_shape(lhs_contracting_shape, rhs_contracting_shape):\n\u001b[32m   5337\u001b[39m   msg = (\u001b[33m\"\u001b[39m\u001b[33mdot_general requires contracting dimensions to have the same \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5338\u001b[39m          \u001b[33m\"\u001b[39m\u001b[33mshape, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m5339\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg.format(lhs_contracting_shape, rhs_contracting_shape))\n\u001b[32m   5341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _dot_general_shape_computation(lhs.shape, rhs.shape, dimension_numbers)\n",
      "\u001b[31mTypeError\u001b[39m: dot_general requires contracting dimensions to have the same shape, got (784,) and (128,)."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# TL;DR\n",
       "\n",
       "**Problem**: Matrix dimension mismatch in `jnp.dot(x, w1)`. You're trying to multiply `x` (32, 784) with `w1` (128, 784), but for matrix multiplication the inner dimensions must match.\n",
       "\n",
       "**Fix**: Transpose `w1` to shape (784, 128):\n",
       "```python\n",
       "h = jnp.dot(x, w1.T) + b1  # or h = jnp.dot(x, w1.T) + b1\n",
       "```\n",
       "\n",
       "Or initialize `w1` with the correct shape from the start:\n",
       "```python\n",
       "w1 = jnp.ones((784, 128))  # Change from (128, 784)\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "# Detailed Analysis\n",
       "\n",
       "## What Went Wrong\n",
       "\n",
       "The error occurs in the `mlp_forward` function at line 10:\n",
       "```python\n",
       "h = jnp.dot(x, w1) + b1\n",
       "```\n",
       "\n",
       "### The Issue\n",
       "\n",
       "You're attempting to perform a matrix multiplication between:\n",
       "- `x`: shape **(32, 784)** - batch of 32 samples, each with 784 features\n",
       "- `w1`: shape **(128, 784)** - weight matrix\n",
       "\n",
       "JAX's `jnp.dot()` function (for 2D arrays) performs standard matrix multiplication, which requires:\n",
       "- For `A @ B`, the last dimension of `A` must match the first dimension of `B`\n",
       "- Here: `(32, 784) @ (128, 784)` attempts to contract dimension 784 from `x` with dimension 128 from `w1`\n",
       "\n",
       "### What the Error Message Tells Us\n",
       "\n",
       "From the error details:\n",
       "```\n",
       "lhs = ShapedArray(float32[32,784])\n",
       "rhs = ShapedArray(float32[128,784])\n",
       "dimension_numbers = (((1,), (0,)), ((), ()))\n",
       "lhs_contracting_shape = (784,)\n",
       "rhs_contracting_shape = (128,)\n",
       "```\n",
       "\n",
       "JAX is trying to contract:\n",
       "- Dimension 1 of `lhs` (size 784)\n",
       "- Dimension 0 of `rhs` (size 128)\n",
       "\n",
       "These don't match: **784 ≠ 128**, hence the error.\n",
       "\n",
       "## Why Your Code Has This Shape\n",
       "\n",
       "Looking at your initialization:\n",
       "```python\n",
       "w1 = jnp.ones((128, 784))  # Hidden layer weights\n",
       "```\n",
       "\n",
       "For a neural network layer transforming 784 input features to 128 hidden units, the weight matrix should be **(784, 128)**, not (128, 784). This way:\n",
       "- Input: (batch_size, 784)\n",
       "- Weights: (784, 128)\n",
       "- Output: (batch_size, 128)\n",
       "\n",
       "## Solutions\n",
       "\n",
       "### Option 1: Fix the Initialization (Recommended)\n",
       "```python\n",
       "w1 = jnp.ones((784, 128))  # Shape: (input_dim, hidden_dim)\n",
       "b1 = jnp.zeros(128)\n",
       "w2 = jnp.ones((128, 10))   # Shape: (hidden_dim, output_dim)\n",
       "b2 = jnp.zeros(10)\n",
       "```\n",
       "\n",
       "### Option 2: Transpose During Forward Pass\n",
       "```python\n",
       "h = jnp.dot(x, w1.T) + b1  # Transpose w1 from (128, 784) to (784, 128)\n",
       "```\n",
       "\n",
       "### Complete Fixed Code\n",
       "```python\n",
       "def mlp_forward(params, x):\n",
       "    \"\"\"Forward pass through a 2-layer MLP.\"\"\"\n",
       "    w1, b1, w2, b2 = params\n",
       "    \n",
       "    # Hidden layer\n",
       "    h = jnp.dot(x, w1) + b1  # (32, 784) @ (784, 128) = (32, 128)\n",
       "    h = jax.nn.relu(h)\n",
       "    \n",
       "    # Output layer\n",
       "    y = jnp.dot(h, w2) + b2  # (32, 128) @ (128, 10) = (32, 10)\n",
       "    return y\n",
       "\n",
       "# Correct initialization\n",
       "w1 = jnp.ones((784, 128))\n",
       "b1 = jnp.zeros(128)\n",
       "w2 = jnp.ones((128, 10))\n",
       "b2 = jnp.zeros(10)\n",
       "params = (w1, b1, w2, b2)\n",
       "batch = jnp.ones((32, 784))\n",
       "\n",
       "mlp_forward(params, batch)  # Now works!\n",
       "```\n",
       "\n",
       "## Key Takeaway\n",
       "\n",
       "In neural networks, weight matrices should have shape `(input_features, output_features)` to allow standard matrix multiplication: `output = input @ weights + bias`."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def mlp_forward(params, x):\n",
    "    \"\"\"Forward pass through a 2-layer MLP.\"\"\"\n",
    "    w1, b1, w2, b2 = params\n",
    "\n",
    "    # Hidden layer\n",
    "    h = jnp.dot(x, w1) + b1\n",
    "    h = jax.nn.relu(h)\n",
    "\n",
    "    # Output layer\n",
    "    out = jnp.dot(h, w2) + b2\n",
    "    return out\n",
    "\n",
    "\n",
    "w1 = jnp.ones((128, 784))\n",
    "b1 = jnp.zeros((128,))\n",
    "w2 = jnp.ones((128, 10))\n",
    "b2 = jnp.zeros((10,))\n",
    "\n",
    "params = (w1, b1, w2, b2)\n",
    "batch = jnp.ones((32, 784))\n",
    "\n",
    "mlp_forward(params, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9864f",
   "metadata": {},
   "source": [
    "## Example: vmap Batch Dimension Error\n",
    "\n",
    "When using `vmap`, errors can be cryptic. The transformation context and array shapes help identify:\n",
    "\n",
    "- Which transformation is active\n",
    "- Whether arrays are tracers\n",
    "- The actual vs expected shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f24ed078",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "vmap got inconsistent sizes for array axes to be mapped:\n  * one axis had size 100: axis 0 of argument x of type float32[100,3];\n  * one axis had size 50: axis 0 of argument y of type float32[50,3]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m     references = pickle.load(f)\n\u001b[32m     17\u001b[39m batched_distance = jax.vmap(pairwise_distance, in_axes=(\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mbatched_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/fincantatem/.venv/lib/python3.12/site-packages/jax/_src/api.py:1327\u001b[39m, in \u001b[36m_mapped_axis_size\u001b[39m\u001b[34m(fn, tree, vals, dims, name)\u001b[39m\n\u001b[32m   1325\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1326\u001b[39m     msg.append(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  * some axes (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mct\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of them) had size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msz\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, e.g. axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00max\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m;\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1327\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(msg)[:-\u001b[32m2\u001b[39m])\n",
      "\u001b[31mValueError\u001b[39m: vmap got inconsistent sizes for array axes to be mapped:\n  * one axis had size 100: axis 0 of argument x of type float32[100,3];\n  * one axis had size 50: axis 0 of argument y of type float32[50,3]"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# TL;DR\n",
       "\n",
       "**Problem:** You're calling `jax.vmap` with `in_axes=(0, 0)` on two arrays with different sizes along axis 0: `queries` has shape `(100, 3)` and `references` has shape `(50, 3)`. JAX's `vmap` requires all mapped axes to have the same size when using the same axis specification.\n",
       "\n",
       "**Fix:** Change your `vmap` usage based on your intent:\n",
       "- For **broadcasting** (compute all pairwise distances): Use `in_axes=(0, None)` and `in_axes=(None, 0)` with nested `vmap`\n",
       "- For **element-wise** distances: Ensure both arrays have the same first dimension (both 100 or both 50)\n",
       "\n",
       "---\n",
       "\n",
       "# Detailed Analysis\n",
       "\n",
       "## What Went Wrong\n",
       "\n",
       "The error occurs at line 19 of your code:\n",
       "```python\n",
       "batched_distance(queries, references)\n",
       "```\n",
       "\n",
       "Where:\n",
       "- `queries` has shape `(100, 3)` \n",
       "- `references` has shape `(50, 3)`\n",
       "- `batched_distance = jax.vmap(pairwise_distance, in_axes=(0, 0))`\n",
       "\n",
       "### Understanding `vmap` with `in_axes=(0, 0)`\n",
       "\n",
       "When you specify `in_axes=(0, 0)`, you're telling JAX to:\n",
       "1. Map over axis 0 of the first argument (`x`)\n",
       "2. Map over axis 0 of the second argument (`y`)\n",
       "3. Apply the function element-wise to corresponding pairs\n",
       "\n",
       "This is similar to a zip operation - JAX expects to pair up `queries[0]` with `references[0]`, `queries[1]` with `references[1]`, etc. For this to work, **both axes must have the same size**.\n",
       "\n",
       "Since you have 100 queries but only 50 references, JAX cannot pair them up and raises a `ValueError`.\n",
       "\n",
       "## Solution Options\n",
       "\n",
       "### Option 1: Compute All Pairwise Distances (Most Likely Intent)\n",
       "\n",
       "If you want to compute the distance between **every** query and **every** reference (resulting in a 100×50 matrix):\n",
       "\n",
       "```python\n",
       "# Double vmap: outer over queries, inner over references\n",
       "batched_distance = jax.vmap(\n",
       "    lambda q: jax.vmap(lambda r: pairwise_distance(q, r))(references)\n",
       ")\n",
       "result = batched_distance(queries)  # Shape: (100, 50)\n",
       "\n",
       "# Or more concisely:\n",
       "batched_distance = jax.vmap(\n",
       "    jax.vmap(pairwise_distance, in_axes=(None, 0)),\n",
       "    in_axes=(0, None)\n",
       ")\n",
       "result = batched_distance(queries, references)  # Shape: (100, 50)\n",
       "```\n",
       "\n",
       "### Option 2: Element-wise Distances\n",
       "\n",
       "If you want element-wise distances (query[i] with reference[i]), make sure both arrays have the same size:\n",
       "\n",
       "```python\n",
       "queries = jnp.ones((50, 3))  # Match the size\n",
       "references = jnp.ones((50, 3))\n",
       "\n",
       "batched_distance = jax.vmap(pairwise_distance, in_axes=(0, 0))\n",
       "result = batched_distance(queries, references)  # Shape: (50,)\n",
       "```\n",
       "\n",
       "### Option 3: Broadcast One Array\n",
       "\n",
       "If you want to compute distances from all queries to a **single** reference (or vice versa):\n",
       "\n",
       "```python\n",
       "# All queries to first reference\n",
       "batched_distance = jax.vmap(pairwise_distance, in_axes=(0, None))\n",
       "result = batched_distance(queries, references[0])  # Shape: (100,)\n",
       "```\n",
       "\n",
       "## Key Takeaway\n",
       "\n",
       "The `in_axes` parameter controls how arrays are mapped:\n",
       "- `in_axes=(0, 0)`: Element-wise mapping (requires same size)\n",
       "- `in_axes=(0, None)`: Map over first arg, broadcast second\n",
       "- `in_axes=(None, 0)`: Broadcast first, map over second\n",
       "- Nested `vmap`: Create Cartesian product of operations"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import pickle\n",
    "import jax\n",
    "\n",
    "\n",
    "def pairwise_distance(x, y):\n",
    "    \"\"\"Compute distance between two points.\"\"\"\n",
    "    return jnp.sqrt(jnp.sum((x - y) ** 2))\n",
    "\n",
    "\n",
    "with open(\"queries.pkl\", \"rb\") as f:\n",
    "    queries = pickle.load(f)\n",
    "\n",
    "with open(\"references.pkl\", \"rb\") as f:\n",
    "    references = pickle.load(f)\n",
    "\n",
    "batched_distance = jax.vmap(pairwise_distance, in_axes=(0, 0))\n",
    "\n",
    "batched_distance(queries, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38a638",
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function loss at /var/folders/jz/dlk_sncn2lvfszqx5d49t0z40000gn/T/ipykernel_49729/1709465187.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTracerBoolConversionError\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 25 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mloss\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(x):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.sum() > \u001b[32m0\u001b[39m:\n\u001b[32m      5\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m x.sum()\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/fincantatem/.venv/lib/python3.12/site-packages/jax/_src/core.py:1831\u001b[39m, in \u001b[36mconcretization_function_error.<locals>.error\u001b[39m\u001b[34m(self, arg)\u001b[39m\n\u001b[32m   1830\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[32m-> \u001b[39m\u001b[32m1831\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[31mTracerBoolConversionError\u001b[39m: Attempted boolean conversion of traced array with shape bool[].\nThe error occurred while tracing the function loss at /var/folders/jz/dlk_sncn2lvfszqx5d49t0z40000gn/T/ipykernel_49729/1709465187.py:3 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\nSee https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# TL;DR\n",
       "\n",
       "**Problem**: You're using a Python `if` statement with a condition that depends on traced JAX array values (`x.sum() > 0`) inside a JIT-compiled function. JAX cannot convert traced values to concrete boolean values during compilation.\n",
       "\n",
       "**Fix**: Replace the Python `if` statement with JAX's functional conditional `jnp.where()`:\n",
       "\n",
       "```python\n",
       "def loss(x):\n",
       "    return jnp.where(x.sum() > 0, x.sum(), 0.0)\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "# Detailed Analysis\n",
       "\n",
       "## What Went Wrong\n",
       "\n",
       "The error occurs in this code:\n",
       "\n",
       "```python\n",
       "def loss(x):\n",
       "    if x.sum() > 0:  # ← Problem here\n",
       "        return x.sum()\n",
       "    else:\n",
       "        return 0.0\n",
       "\n",
       "jax.grad(jax.jit(loss))(jnp.array([1.0, -2.0, 1.0]))\n",
       "```\n",
       "\n",
       "### Root Cause\n",
       "\n",
       "JAX uses **tracing** to compile functions with `jit`. During tracing:\n",
       "\n",
       "1. JAX replaces concrete values with abstract \"tracer\" objects that track operations\n",
       "2. These tracers represent the **shape and dtype** of values, not their actual content\n",
       "3. Python's `if` statement requires a concrete `True` or `False` value\n",
       "4. When you write `if x.sum() > 0:`, JAX tries to convert the traced boolean to a Python bool\n",
       "5. This fails because the actual value isn't known yet—only the computation graph is being built\n",
       "\n",
       "From the stack trace, you can see:\n",
       "- **Frame 1** shows the error at line 4: `if x.sum() > 0:`\n",
       "- **Immediate failure** shows: `TracerBoolConversionError` with `JitTracer<bool[]>` (a traced boolean, not a concrete value)\n",
       "\n",
       "### Why This Happens with JIT\n",
       "\n",
       "The error specifically mentions \"while tracing the function loss... for jit\". When you use `jax.grad(jax.jit(loss))`:\n",
       "\n",
       "1. `jax.jit` tries to compile the function to XLA\n",
       "2. During compilation, it traces through your code with abstract values\n",
       "3. The `if` statement forces Python to evaluate the boolean, which isn't possible with tracers\n",
       "\n",
       "## The Solution\n",
       "\n",
       "JAX provides functional alternatives to Python control flow:\n",
       "\n",
       "### Option 1: `jnp.where()` (recommended for simple cases)\n",
       "\n",
       "```python\n",
       "def loss(x):\n",
       "    return jnp.where(x.sum() > 0, x.sum(), 0.0)\n",
       "```\n",
       "\n",
       "This evaluates both branches and selects the result based on the condition.\n",
       "\n",
       "### Option 2: `jax.lax.cond()` (for complex branches)\n",
       "\n",
       "```python\n",
       "import jax.lax as lax\n",
       "\n",
       "def loss(x):\n",
       "    return lax.cond(\n",
       "        x.sum() > 0,\n",
       "        lambda x: x.sum(),\n",
       "        lambda x: 0.0,\n",
       "        x\n",
       "    )\n",
       "```\n",
       "\n",
       "This is more efficient when branches have expensive computations, as only one branch executes.\n",
       "\n",
       "### Option 3: Remove `jit` (not recommended)\n",
       "\n",
       "```python\n",
       "jax.grad(loss)(jnp.array([1.0, -2.0, 1.0]))  # Works but slow\n",
       "```\n",
       "\n",
       "This works but defeats the purpose of using JAX for performance.\n",
       "\n",
       "## Key Takeaways\n",
       "\n",
       "1. **Never use Python `if/elif/else` with traced JAX arrays** inside jitted functions\n",
       "2. Use JAX's functional equivalents: `jnp.where()`, `lax.cond()`, `lax.switch()`\n",
       "3. Similarly, avoid Python `for`/`while` loops—use `jax.lax.fori_loop()` or `jax.lax.scan()`\n",
       "4. This is a fundamental constraint of JAX's compilation model, not a bug"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jax, jax.numpy as jnp\n",
    "\n",
    "\n",
    "def loss(x):\n",
    "    if x.sum() > 0:\n",
    "        return x.sum()\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "jax.grad(jax.jit(loss))(jnp.array([1.0, -2.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1be3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def normalize_row(x):\n",
    "    return x / jnp.linalg.norm(x, axis=0)\n",
    "\n",
    "\n",
    "def batched_normalize(X):\n",
    "    # Intended X: (batch, d)\n",
    "    return jax.vmap(normalize_row)(X)\n",
    "\n",
    "\n",
    "X_good = jnp.ones((4, 8))\n",
    "X_bad = jnp.ones((4, 8)).sum(axis=1)\n",
    "\n",
    "jax.jit(batched_normalize)(X_bad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
